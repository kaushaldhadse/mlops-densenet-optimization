{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfdf154",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python_Programs\\SatSure\\mlops-densenet-optimization\\venv\\Lib\\site-packages\\torch\\cuda\\__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch built with CUDA: 12.6\n",
      "GPU compute capability: (7, 5)\n",
      "GPU name: NVIDIA GeForce GTX 1650 Ti with Max-Q Design\n",
      "CUDA runtime version: 12.6\n",
      "Is CUDA available? True\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# print(\"PyTorch built with CUDA:\", torch.version.cuda)\n",
    "\n",
    "# print(\"GPU compute capability:\", torch.cuda.get_device_capability(0) if torch.cuda.is_available() else \"No GPU detected\")\n",
    "# print(\"GPU name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU detected\")\n",
    "\n",
    "# print(\"CUDA runtime version:\", torch.version.cuda)\n",
    "# print(\"Is CUDA available?\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98b977cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running benchmarks on device: cuda\n",
      "2.8.0+cu126\n",
      "12.6\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Running benchmarks on device: {DEVICE}\")\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b78b1168",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python_Programs\\SatSure\\mlops-densenet-optimization\\venv\\Lib\\site-packages\\torch\\cuda\\__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    }
   ],
   "source": [
    "# from app.utils import get_ram_usage_mb\n",
    "\n",
    "\n",
    "# import sys\n",
    "# import os\n",
    "\n",
    "# # Add project root (two levels up from notebooks/)\n",
    "# project_root = os.path.abspath(os.path.join(os.getcwd(), \"../..\"))\n",
    "# sys.path.append(project_root)\n",
    "\n",
    "import utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b826071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running benchmarks on device: cuda\n",
      "\n",
      "Benchmarking batch size = 1\n",
      "\n",
      "Benchmarking batch size = 4\n",
      "\n",
      "Benchmarking batch size = 8\n",
      "\n",
      "Benchmarking batch size = 16\n",
      "\n",
      "Benchmarking batch size = 32\n",
      "\n",
      "✅ Results saved to results\\benchmark_results.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import pandas as pd\n",
    "import psutil\n",
    "import utils  # our helper functions (in app/utils.py)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# CONFIG\n",
    "# -----------------------------\n",
    "BATCH_SIZES = [1, 4, 8, 16, 32]\n",
    "INPUT_SHAPE = (3, 224, 224)\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "RESULTS_CSV = os.path.join(\"results\", \"benchmark_results.csv\")\n",
    "LOG_DIR = os.path.join(\"logs\", \"tensorboard\")\n",
    "\n",
    "\n",
    "def benchmark_densenet():\n",
    "    os.makedirs(\"results\", exist_ok=True)\n",
    "    os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "    writer = SummaryWriter(LOG_DIR)\n",
    "    results = []\n",
    "\n",
    "    print(f\"Running benchmarks on device: {DEVICE}\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # Load model\n",
    "    # -----------------------------\n",
    "    start_time = time.time()\n",
    "    model = models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1).to(DEVICE)  # no pretrained weights\n",
    "    model.eval()\n",
    "    model_load_time = (time.time() - start_time) * 1000  # ms\n",
    "\n",
    "    # -----------------------------\n",
    "    # Run for each batch size\n",
    "    # -----------------------------\n",
    "    for batch_size in BATCH_SIZES:\n",
    "        print(f\"\\nBenchmarking batch size = {batch_size}\")\n",
    "        inputs = torch.randn(batch_size, *INPUT_SHAPE).to(DEVICE)\n",
    "\n",
    "        # warmup\n",
    "        with torch.no_grad():\n",
    "            for _ in range(3):\n",
    "                _ = model(inputs)\n",
    "\n",
    "        # profiling\n",
    "        with profile(\n",
    "            activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA] if DEVICE == \"cuda\" else [ProfilerActivity.CPU],\n",
    "            record_shapes=True,\n",
    "            profile_memory=True,\n",
    "            with_stack=True\n",
    "        ) as prof:\n",
    "            with record_function(\"model_inference\"):\n",
    "                torch.cuda.synchronize() if DEVICE == \"cuda\" else None\n",
    "                start = time.time()\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(inputs)\n",
    "                torch.cuda.synchronize() if DEVICE == \"cuda\" else None\n",
    "                end = time.time()\n",
    "\n",
    "        # latency and throughput\n",
    "        latency_ms = (end - start) * 1000\n",
    "        throughput = batch_size / (end - start)\n",
    "\n",
    "        # system stats\n",
    "        ram_usage = utils.get_ram_usage_mb()\n",
    "        vram_usage = utils.get_vram_usage_mb() if DEVICE == \"cuda\" else 0\n",
    "        cpu_util = utils.get_cpu_utilization()\n",
    "        gpu_util = utils.get_gpu_utilization() if DEVICE == \"cuda\" else 0\n",
    "\n",
    "        # log to tensorboard\n",
    "        writer.add_scalar(f\"Latency/batch_{batch_size}\", latency_ms)\n",
    "        writer.add_scalar(f\"Throughput/batch_{batch_size}\", throughput)\n",
    "        writer.add_scalar(f\"RAM_Usage_MB/batch_{batch_size}\", ram_usage)\n",
    "        if DEVICE == \"cuda\":\n",
    "            writer.add_scalar(f\"VRAM_Usage_MB/batch_{batch_size}\", vram_usage)\n",
    "            writer.add_scalar(f\"GPU_Utilization/batch_{batch_size}\", gpu_util)\n",
    "\n",
    "        # append to results\n",
    "        results.append({\n",
    "            \"model_variant\": \"densenet121_baseline\",\n",
    "            \"batch_size\": batch_size,\n",
    "            \"device\": DEVICE,\n",
    "            \"ram_usage_mb\": ram_usage,\n",
    "            \"vram_usage_mb\": vram_usage,\n",
    "            \"cpu_utilization_pct\": cpu_util,\n",
    "            \"gpu_utilization_pct\": gpu_util,\n",
    "            \"latency_ms\": latency_ms,\n",
    "            \"throughput_samples_sec\": throughput,\n",
    "            \"accuracy_top1\": \"NA\",  # will be added in Part 2\n",
    "            \"accuracy_top5\": \"NA\",\n",
    "            \"model_size_mb\": utils.get_model_size_mb(model),\n",
    "            \"optimization_technique\": \"baseline\",\n",
    "            \"model_load_time_ms\": model_load_time\n",
    "        })\n",
    "\n",
    "    writer.close()\n",
    "\n",
    "    # save CSV\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(RESULTS_CSV, index=False)\n",
    "    print(f\"\\n✅ Results saved to {RESULTS_CSV}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    benchmark_densenet()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
