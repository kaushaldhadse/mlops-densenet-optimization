{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfdf154",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python_Programs\\SatSure\\mlops-densenet-optimization\\venv\\Lib\\site-packages\\torch\\cuda\\__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch built with CUDA: 12.6\n",
      "GPU compute capability: (7, 5)\n",
      "GPU name: NVIDIA GeForce GTX 1650 Ti with Max-Q Design\n",
      "CUDA runtime version: 12.6\n",
      "Is CUDA available? True\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# print(\"PyTorch built with CUDA:\", torch.version.cuda)\n",
    "\n",
    "# print(\"GPU compute capability:\", torch.cuda.get_device_capability(0) if torch.cuda.is_available() else \"No GPU detected\")\n",
    "# print(\"GPU name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU detected\")\n",
    "\n",
    "# print(\"CUDA runtime version:\", torch.version.cuda)\n",
    "# print(\"Is CUDA available?\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98b977cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running benchmarks on device: cuda\n",
      "2.8.0+cu126\n",
      "12.6\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Running benchmarks on device: {DEVICE}\")\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b78b1168",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python_Programs\\SatSure\\mlops-densenet-optimization\\venv\\Lib\\site-packages\\torch\\cuda\\__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    }
   ],
   "source": [
    "# from app.utils import get_ram_usage_mb\n",
    "\n",
    "\n",
    "# import sys\n",
    "# import os\n",
    "\n",
    "# # Add project root (two levels up from notebooks/)\n",
    "# project_root = os.path.abspath(os.path.join(os.getcwd(), \"../..\"))\n",
    "# sys.path.append(project_root)\n",
    "\n",
    "import utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b826071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running benchmarks on device: cuda\n",
      "\n",
      "Benchmarking batch size = 1\n",
      "\n",
      "Benchmarking batch size = 4\n",
      "\n",
      "Benchmarking batch size = 8\n",
      "\n",
      "Benchmarking batch size = 16\n",
      "\n",
      "Benchmarking batch size = 32\n",
      "\n",
      "✅ Results saved to results\\benchmark_results.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import pandas as pd\n",
    "import psutil\n",
    "import utils  # our helper functions (in app/utils.py)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# CONFIG\n",
    "# -----------------------------\n",
    "BATCH_SIZES = [1, 4, 8, 16, 32]\n",
    "INPUT_SHAPE = (3, 224, 224)\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "RESULTS_CSV = os.path.join(\"results\", \"benchmark_results.csv\")\n",
    "LOG_DIR = os.path.join(\"logs\", \"tensorboard\")\n",
    "\n",
    "\n",
    "def benchmark_densenet():\n",
    "    os.makedirs(\"results\", exist_ok=True)\n",
    "    os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "    writer = SummaryWriter(LOG_DIR)\n",
    "    results = []\n",
    "\n",
    "    print(f\"Running benchmarks on device: {DEVICE}\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # Load model\n",
    "    # -----------------------------\n",
    "    start_time = time.time()\n",
    "    model = models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1).to(DEVICE)  # no pretrained weights\n",
    "    model.eval()\n",
    "    model_load_time = (time.time() - start_time) * 1000  # ms\n",
    "\n",
    "    # -----------------------------\n",
    "    # Run for each batch size\n",
    "    # -----------------------------\n",
    "    for batch_size in BATCH_SIZES:\n",
    "        print(f\"\\nBenchmarking batch size = {batch_size}\")\n",
    "        inputs = torch.randn(batch_size, *INPUT_SHAPE).to(DEVICE)\n",
    "\n",
    "        # warmup\n",
    "        with torch.no_grad():\n",
    "            for _ in range(3):\n",
    "                _ = model(inputs)\n",
    "\n",
    "        # profiling\n",
    "        with profile(\n",
    "            activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA] if DEVICE == \"cuda\" else [ProfilerActivity.CPU],\n",
    "            record_shapes=True,\n",
    "            profile_memory=True,\n",
    "            with_stack=True\n",
    "        ) as prof:\n",
    "            with record_function(\"model_inference\"):\n",
    "                torch.cuda.synchronize() if DEVICE == \"cuda\" else None\n",
    "                start = time.time()\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(inputs)\n",
    "                torch.cuda.synchronize() if DEVICE == \"cuda\" else None\n",
    "                end = time.time()\n",
    "\n",
    "        # latency and throughput\n",
    "        latency_ms = (end - start) * 1000\n",
    "        throughput = batch_size / (end - start)\n",
    "\n",
    "        # system stats\n",
    "        ram_usage = utils.get_ram_usage_mb()\n",
    "        vram_usage = utils.get_vram_usage_mb() if DEVICE == \"cuda\" else 0\n",
    "        cpu_util = utils.get_cpu_utilization()\n",
    "        gpu_util = utils.get_gpu_utilization() if DEVICE == \"cuda\" else 0\n",
    "\n",
    "        # log to tensorboard\n",
    "        writer.add_scalar(f\"Latency/batch_{batch_size}\", latency_ms)\n",
    "        writer.add_scalar(f\"Throughput/batch_{batch_size}\", throughput)\n",
    "        writer.add_scalar(f\"RAM_Usage_MB/batch_{batch_size}\", ram_usage)\n",
    "        if DEVICE == \"cuda\":\n",
    "            writer.add_scalar(f\"VRAM_Usage_MB/batch_{batch_size}\", vram_usage)\n",
    "            writer.add_scalar(f\"GPU_Utilization/batch_{batch_size}\", gpu_util)\n",
    "\n",
    "        # append to results\n",
    "        results.append({\n",
    "            \"model_variant\": \"densenet121_baseline\",\n",
    "            \"batch_size\": batch_size,\n",
    "            \"device\": DEVICE,\n",
    "            \"ram_usage_mb\": ram_usage,\n",
    "            \"vram_usage_mb\": vram_usage,\n",
    "            \"cpu_utilization_pct\": cpu_util,\n",
    "            \"gpu_utilization_pct\": gpu_util,\n",
    "            \"latency_ms\": latency_ms,\n",
    "            \"throughput_samples_sec\": throughput,\n",
    "            \"accuracy_top1\": \"NA\",  # will be added in Part 2\n",
    "            \"accuracy_top5\": \"NA\",\n",
    "            \"model_size_mb\": utils.get_model_size_mb(model),\n",
    "            \"optimization_technique\": \"baseline\",\n",
    "            \"model_load_time_ms\": model_load_time\n",
    "        })\n",
    "\n",
    "    writer.close()\n",
    "\n",
    "    # save CSV\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(RESULTS_CSV, index=False)\n",
    "    print(f\"\\n✅ Results saved to {RESULTS_CSV}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    benchmark_densenet()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2742d3ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3964f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "optimization: baseline and batch size : 1\n",
      "optimization: baseline and batch size : 4\n",
      "optimization: baseline and batch size : 8\n",
      "optimization: baseline and batch size : 16\n",
      "optimization: baseline and batch size : 32\n",
      "optimization: amp and batch size : 1\n",
      "optimization: amp and batch size : 4\n",
      "optimization: amp and batch size : 8\n",
      "optimization: amp and batch size : 16\n",
      "optimization: amp and batch size : 32\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 94\u001b[39m\n\u001b[32m     90\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[INFO] Benchmarking complete. Results saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult_csv\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     93\u001b[39m \u001b[38;5;66;03m# if __name__ == \"__main__\":\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[43mbenchmark_densenet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 78\u001b[39m, in \u001b[36mbenchmark_densenet\u001b[39m\u001b[34m(batch_sizes, device)\u001b[39m\n\u001b[32m     75\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33moptimization: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mopt\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m and batch size : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     77\u001b[39m \u001b[38;5;66;03m# Save results\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m project_root = os.path.abspath(os.path.join(os.path.dirname(\u001b[34;43m__file__\u001b[39;49m), \u001b[33m\"\u001b[39m\u001b[33m..\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m     79\u001b[39m results_dir = os.path.join(project_root, \u001b[33m\"\u001b[39m\u001b[33mresults\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     80\u001b[39m os.makedirs(results_dir, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "from torchvision import models\n",
    "import utils\n",
    "import optimisations  # NEW\n",
    "\n",
    "# Add optimization techniques\n",
    "# OPTIMIZATIONS = [\"baseline\", \"amp\", \"jit\", \"quantization\"]\n",
    "OPTIMIZATIONS = [\"baseline\", \"amp\"]\n",
    "\n",
    "def benchmark_densenet(batch_sizes=[1, 4, 8, 16, 32], device=None):\n",
    "    results = []\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(device) \n",
    "    sample_input = torch.randn(1, 3, 224, 224).to(device)\n",
    "\n",
    "    for opt in OPTIMIZATIONS:\n",
    "        for batch_size in batch_sizes:\n",
    "            input_tensor = torch.randn(batch_size, 3, 224, 224).to(device)\n",
    "\n",
    "            # Load model with optimization\n",
    "            model = optimisations.get_model(opt, device, sample_input)\n",
    "            model.eval()\n",
    "\n",
    "            # Warmup\n",
    "            for _ in range(5):\n",
    "                with torch.no_grad():\n",
    "                    if opt == \"amp\" and device == \"cuda\":\n",
    "                        with torch.amp.autocast(\"cuda\"):\n",
    "                            _ = model(input_tensor)\n",
    "                    else:\n",
    "                        _ = model(input_tensor)\n",
    "\n",
    "            # Measure inference time\n",
    "            torch.cuda.synchronize() if device == \"cuda\" else None\n",
    "            start_time = time.time()\n",
    "            with torch.no_grad():\n",
    "                if opt == \"amp\" and device == \"cuda\":\n",
    "                    with torch.amp.autocast(\"cuda\"):\n",
    "                        _ = model(input_tensor)\n",
    "                else:\n",
    "                    _ = model(input_tensor)\n",
    "            torch.cuda.synchronize() if device == \"cuda\" else None\n",
    "            latency = (time.time() - start_time) * 1000  # ms\n",
    "\n",
    "            throughput = batch_size / (latency / 1000)\n",
    "\n",
    "            # Collect metrics\n",
    "            ram_usage = utils.get_ram_usage_mb()\n",
    "            vram_usage = utils.get_vram_usage_mb() if device == \"cuda\" else None\n",
    "            cpu_util = utils.get_cpu_utilization()\n",
    "            gpu_util = utils.get_gpu_utilization() if device == \"cuda\" else None\n",
    "            model_size_mb = utils.get_model_size_mb(model)\n",
    "\n",
    "            results.append({\n",
    "                \"model_variant\": \"densenet121\",\n",
    "                \"batch_size\": batch_size,\n",
    "                \"device\": device,\n",
    "                \"ram_usage_mb\": ram_usage,\n",
    "                \"vram_usage_mb\": vram_usage,\n",
    "                \"cpu_utilization_pct\": cpu_util,\n",
    "                \"gpu_utilization_pct\": gpu_util,\n",
    "                \"latency_ms\": latency,\n",
    "                \"throughput_samples_sec\": throughput,\n",
    "                \"accuracy_top1\": None,   # still NA until dataset added\n",
    "                \"accuracy_top5\": None,   # still NA\n",
    "                \"model_size_mb\": model_size_mb,\n",
    "                \"optimization_technique\": opt,\n",
    "            })\n",
    "            \n",
    "            print(f\"optimization: {opt} and batch size : {batch_size}\")\n",
    "\n",
    "    # Save results\n",
    "    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\"))\n",
    "    results_dir = os.path.join(project_root, \"results\")\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "    result_csv = os.path.join(results_dir, \"benchmark_results.csv\")\n",
    "    df = pd.DataFrame(results)\n",
    "\n",
    "    # overwrite safe\n",
    "    if os.path.exists(result_csv):\n",
    "        os.remove(result_csv)\n",
    "    df.to_csv(result_csv, index=False)\n",
    "\n",
    "    print(f\"[INFO] Benchmarking complete. Results saved to {result_csv}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    benchmark_densenet()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b67d812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Running benchmarks on device: cuda\n",
      "\n",
      "[INFO] === Optimization: quantization ===\n",
      "[INFO] Benchmarking batch size = 1\n",
      "[DONE] quantization | batch=1 | latency=118.00ms | throughput=8.47/s\n",
      "[INFO] Benchmarking batch size = 4\n",
      "[DONE] quantization | batch=4 | latency=428.99ms | throughput=9.32/s\n",
      "[INFO] Benchmarking batch size = 8\n",
      "[DONE] quantization | batch=8 | latency=805.00ms | throughput=9.94/s\n",
      "[INFO] Benchmarking batch size = 16\n",
      "[DONE] quantization | batch=16 | latency=1464.00ms | throughput=10.93/s\n",
      "[INFO] Benchmarking batch size = 32\n",
      "[DONE] quantization | batch=32 | latency=3228.00ms | throughput=9.91/s\n",
      "\n",
      "[INFO] === Optimization: baseline ===\n",
      "[INFO] Benchmarking batch size = 1\n",
      "[DONE] baseline | batch=1 | latency=88.45ms | throughput=11.31/s\n",
      "[INFO] Benchmarking batch size = 4\n",
      "[DONE] baseline | batch=4 | latency=51.00ms | throughput=78.43/s\n",
      "[INFO] Benchmarking batch size = 8\n",
      "[DONE] baseline | batch=8 | latency=63.02ms | throughput=126.95/s\n",
      "[INFO] Benchmarking batch size = 16\n",
      "[DONE] baseline | batch=16 | latency=127.00ms | throughput=125.98/s\n",
      "[INFO] Benchmarking batch size = 32\n",
      "[DONE] baseline | batch=32 | latency=215.00ms | throughput=148.84/s\n",
      "\n",
      "[INFO] === Optimization: amp ===\n",
      "[INFO] Benchmarking batch size = 1\n",
      "[DONE] amp | batch=1 | latency=62.00ms | throughput=16.13/s\n",
      "[INFO] Benchmarking batch size = 4\n",
      "[DONE] amp | batch=4 | latency=131.00ms | throughput=30.53/s\n",
      "[INFO] Benchmarking batch size = 8\n",
      "[DONE] amp | batch=8 | latency=256.05ms | throughput=31.24/s\n",
      "[INFO] Benchmarking batch size = 16\n",
      "[DONE] amp | batch=16 | latency=471.95ms | throughput=33.90/s\n",
      "[INFO] Benchmarking batch size = 32\n",
      "[DONE] amp | batch=32 | latency=918.03ms | throughput=34.86/s\n",
      "\n",
      "✅ Benchmarking complete. Results saved to c:\\Python_Programs\\SatSure\\mlops-densenet-optimization\\app\\results\\benchmark_results.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import models\n",
    "import utils\n",
    "import optimisations  # NEW\n",
    "\n",
    "# Which optimization techniques to benchmark\n",
    "OPTIMIZATIONS = [\"quantization\", \"baseline\", \"amp\"]  # add \"jit\", \"quantization\" later if needed\n",
    "BATCH_SIZES = [1, 4, 8, 16, 32]\n",
    "\n",
    "def benchmark_densenet(batch_sizes=BATCH_SIZES, device=None):\n",
    "    results = []\n",
    "\n",
    "    # -----------------------------\n",
    "    # Setup device\n",
    "    # -----------------------------\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"[INFO] Running benchmarks on device: {device}\")\n",
    "\n",
    "    # Setup TensorBoard writer\n",
    "    try:\n",
    "        project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\"))\n",
    "    except NameError:\n",
    "        # fallback for Jupyter notebooks\n",
    "        project_root = os.getcwd()\n",
    "        \n",
    "    log_dir = os.path.join(project_root, \"logs\", \"tensorboard\")\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    writer = SummaryWriter(log_dir)\n",
    "\n",
    "    # Sample input for optimisations (needed by JIT / AMP)\n",
    "    sample_input = torch.randn(1, 3, 224, 224).to(device)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Loop over optimizations\n",
    "    # -----------------------------\n",
    "    for opt in OPTIMIZATIONS:\n",
    "        print(f\"\\n[INFO] === Optimization: {opt} ===\")\n",
    "\n",
    "        # Load model once per optimization\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # model = optimisations.get_model(opt, device, sample_input)\n",
    "        model_full = optimisations.get_model(\"baseline\", device, sample_input)  # get plain model first\n",
    "        if opt == \"quantization\":\n",
    "            model = optimisations.apply_quantization(model_full.cpu())  # quantize on CPU\n",
    "            run_device = \"cpu\"\n",
    "        else:\n",
    "            model = optimisations.get_model(opt, device, sample_input)\n",
    "            run_device = device\n",
    "\n",
    "        model.eval()\n",
    "        model_load_time = (time.time() - start_time) * 1000  # ms\n",
    "\n",
    "        for batch_size in batch_sizes:\n",
    "            print(f\"[INFO] Benchmarking batch size = {batch_size}\")\n",
    "            input_tensor = torch.randn(batch_size, 3, 224, 224).to(run_device)\n",
    "\n",
    "            # Warmup\n",
    "            with torch.no_grad():\n",
    "                for _ in range(3):\n",
    "                    if opt == \"amp\" and device == \"cuda\":\n",
    "                        with torch.amp.autocast(\"cuda\"):\n",
    "                            _ = model(input_tensor)\n",
    "                    else:\n",
    "                        _ = model(input_tensor)\n",
    "\n",
    "            # Profiling + inference\n",
    "            with profile(\n",
    "                activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA] if device == \"cuda\" else [ProfilerActivity.CPU],\n",
    "                record_shapes=True,\n",
    "                profile_memory=True,\n",
    "                with_stack=True\n",
    "            ) as prof:\n",
    "                with record_function(\"model_inference\"):\n",
    "                    torch.cuda.synchronize() if device == \"cuda\" else None\n",
    "                    start = time.time()\n",
    "                    with torch.no_grad():\n",
    "                        if opt == \"amp\" and device == \"cuda\":\n",
    "                            with torch.amp.autocast(\"cuda\"):\n",
    "                                outputs = model(input_tensor)\n",
    "                        else:\n",
    "                            outputs = model(input_tensor)\n",
    "                    torch.cuda.synchronize() if device == \"cuda\" else None\n",
    "                    end = time.time()\n",
    "\n",
    "            # Latency & throughput\n",
    "            latency_ms = (end - start) * 1000\n",
    "            throughput = batch_size / (end - start)\n",
    "\n",
    "            # Collect metrics\n",
    "            ram_usage = utils.get_ram_usage_mb()\n",
    "            vram_usage = utils.get_vram_usage_mb() if device == \"cuda\" else None\n",
    "            cpu_util = utils.get_cpu_utilization()\n",
    "            gpu_util = utils.get_gpu_utilization() if device == \"cuda\" else None\n",
    "            model_size_mb = utils.get_model_size_mb(model)\n",
    "\n",
    "            # Log to TensorBoard\n",
    "            writer.add_scalar(f\"{opt}/Latency_batch_{batch_size}\", latency_ms)\n",
    "            writer.add_scalar(f\"{opt}/Throughput_batch_{batch_size}\", throughput)\n",
    "            writer.add_scalar(f\"{opt}/RAM_MB_batch_{batch_size}\", ram_usage)\n",
    "            if device == \"cuda\":\n",
    "                writer.add_scalar(f\"{opt}/VRAM_MB_batch_{batch_size}\", vram_usage)\n",
    "                writer.add_scalar(f\"{opt}/GPU_util_batch_{batch_size}\", gpu_util)\n",
    "\n",
    "            # Append to results\n",
    "            results.append({\n",
    "                \"model_variant\": \"densenet121\",\n",
    "                \"batch_size\": batch_size,\n",
    "                \"device\": device,\n",
    "                \"ram_usage_mb\": ram_usage,\n",
    "                \"vram_usage_mb\": vram_usage,\n",
    "                \"cpu_utilization_pct\": cpu_util,\n",
    "                \"gpu_utilization_pct\": gpu_util,\n",
    "                \"latency_ms\": latency_ms,\n",
    "                \"throughput_samples_sec\": throughput,\n",
    "                \"accuracy_top1\": None,   # will be added in Part 2\n",
    "                \"accuracy_top5\": None,\n",
    "                \"model_size_mb\": model_size_mb,\n",
    "                \"optimization_technique\": opt,\n",
    "                \"model_load_time_ms\": model_load_time,\n",
    "            })\n",
    "\n",
    "            print(f\"[DONE] {opt} | batch={batch_size} | \"\n",
    "                  f\"latency={latency_ms:.2f}ms | throughput={throughput:.2f}/s\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # Save results\n",
    "    # -----------------------------\n",
    "    results_dir = os.path.join(project_root, \"results\")\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    result_csv = os.path.join(results_dir, \"benchmark_results.csv\")\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    if os.path.exists(result_csv):\n",
    "        os.remove(result_csv)\n",
    "    df.to_csv(result_csv, index=False)\n",
    "\n",
    "    writer.close()\n",
    "    print(f\"\\n✅ Benchmarking complete. Results saved to {result_csv}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    benchmark_densenet()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
